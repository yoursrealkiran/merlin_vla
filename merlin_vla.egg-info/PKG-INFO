Metadata-Version: 2.4
Name: merlin-vla
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: matplotlib>=3.10.8
Requires-Dist: numpy>=2.4.2
Requires-Dist: pyyaml>=6.0.3
Requires-Dist: sentencepiece>=0.2.1
Requires-Dist: torch>=2.0.0
Requires-Dist: torchvision>=0.15.0
Requires-Dist: tqdm>=4.67.3
Requires-Dist: transformers>=4.44.0
Requires-Dist: wandb>=0.24.2
Dynamic: license-file

# MERLIN: Multimodal Embedding Reasoning for Low-level Imitation and Navigation

MERLIN is a **Visionâ€“Languageâ€“Action (VLA)**-style transformer
implemented in **PyTorch**.

It takes as input:
- an RGB image of the scene  
- a natural language instruction  
- proprioceptive state (e.g., end-effector position)  

and predicts a **continuous action** (e.g., Î”x, Î”y) for a toy reaching task.

The goal of this project is to **study multimodal fusion and transformer-based policies**
on a small, fully reproducible benchmark that runs on a laptop.

---

## ðŸ” Research Questions

MERLIN is designed to explore:

- How well can a small multimodal transformer learn a reaching policy?
- Does adding language instruction help, even when itâ€™s simple?
- What is the effect of:
  - RGB vs RGB + depth
  - Frozen vs trainable text encoder
  - Number of transformer layers / heads
  - Different token orderings (e.g., `[readout, proprio, text, image]`)

---

## ðŸ§± Architecture

High-level architecture:

```text
        +-----------------+        +-------------------+
image ->| ImageTokenizer  |----+   |                   |
        +-----------------+    |   |                   |
                               +-->|                   |
text -->[ T5 Encoder + proj ]--+   | Merlin Transformer|--> [readout token] -> Action MLP
                               +-->|   Backbone        |
proprio->[ Proprio MLP ]-------+   |                   |
                                   |                   |
[learnable readout token] -------->+-------------------+
